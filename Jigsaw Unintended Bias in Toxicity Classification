2019 Kaggle competition
Jigsaw Unintended Bias in Toxicity Classification

06/08 issue
- 결측치, correlation 높은 것 확인
- keras보다 pytorch로 하면 빠름
- cleansing 정도의 따른 성능(####과 같은 것도 의미가 있을 수 있음 -> spelling error와 축약형만 처리, 
                                                                소문자로 통일하는 것을 뺏더니 성능이 더 좋은 경우가 있음)
- time문제 -> pickle, train data set의 memory줄이기

https://www.kaggle.com/timon88/bert-lstm-simple-blender-0-93844-lb/data
https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version
https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai/notebook
def reduce_mem_usage(df):
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df

======================================================================================================================
06/08 issue
- 결측치, correlation 높은 것 확인
- keras보다 pytorch로 하면 빠름
- cleansing 정도의 따른 성능(####과 같은 것도 의미가 있을 수 있음 -> spelling error와 축약형만 처리, 
                                                                소문자로 통일하는 것을 뺏더니 성능이 더 좋은 경우가 있음)
- time문제 -> pickle, train data set의 memory줄이기

1. cnn baseline -> 0.93107  (https://www.kaggle.com/hallohj/jigsaw-cnn-3)
             embedding = fasttext-crawl-300d-2m, glove840b300dtxt
             preprocess = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~`" + '""“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\×™√²—–&'
             BATCH_SIZE = 512
             LSTM_UNITS = 128
             DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS
             EPOCHS = 4
             MAX_LEN = 256
             
2. cnn (ephoch3, train['target'].values>=0.5) -> 0.93047  (https://www.kaggle.com/hallohj/jigsaw-cnn)
             sampling = (frac=0.9,random_state=1234)
             embedding = fasttext-crawl-300d-2m, glove840b300dtxt
             EPOCHS = 3
             MAX_LEN = 220



1. bert + lstm baseline -> 0.93845     (https://www.kaggle.com/hallohj/jigsaw3-bert-lstm)
             embedding = crawl-300d-2M.gensim, glove.840B.300d.gensim
             bert model : MAX_SEQUENCE_LENGTH = 300
                          BATCH_SIZE = 512
                          LSTM_UNITS = 128
                          DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS
                          MAX_LEN = 220

             LSTM model : 전처리 = symbols_to_isolate, symbols_to_delete
                          max_features = 410047
                          maxlen = 300
                          batch_size = 512
             
             blending weight : (bert - 0.333), (lstm - 0.667)
             
2. bert + lstm (blending weight change) -> 0.93844     
             blending weight : (bert - 0.5), (lstm - 0.5)
             
3. bert + lstm (blending weight change) -> 0.93801    
             blending weight : (bert - 0.2), (lstm - 0.8)
             
3. bert + lstm (blending weight change) -> 0.93685   
             blending weight : (bert - 0.8), (lstm - 0.2)
             
4. bert + lstm (blending weight change) -> 0.93849     
             blending weight : (bert - 0.35), (lstm - 0.65)
             
5. bert + lstm (blending weight change, bert_MAX_LEN = 300) -> 0.93852  
blending weight : (bert - 0.36), (lstm - 0.64)

★6. bert + lstm (blending weight change, bert_MAX_LEN = 300) -> 0.93853  
blending weight : (bert - 0.37), (lstm - 0.63)
